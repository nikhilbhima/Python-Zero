{
  "id": "08-common-ai-patterns",
  "title": "Common AI Patterns",
  "description": "Master the design patterns used in professional AI applications",
  "track": "ai-tools",
  "difficulty": "intermediate",
  "estimatedTime": "25 minutes",
  "concepts": ["design patterns", "caching", "streaming", "batch processing"],

  "content": [
    {
      "type": "text",
      "text": "Professional AI applications follow specific design patterns. Learn these patterns, and you can build ANY AI tool! These are the same patterns used by companies building AI products."
    },

    {
      "type": "text",
      "text": "PATTERN 1: CACHING (DON'T REPEAT EXPENSIVE CALLS)\n\nIf you ask the same question twice, why pay twice? Cache results!"
    },

    {
      "type": "code",
      "code": "# âœ… Caching pattern:\n\nimport hashlib\nimport json\nfrom datetime import datetime, timedelta\n\nclass AICache:\n    \"\"\"Cache AI responses to save cost and time.\"\"\"\n    \n    def __init__(self, ttl_hours=24):\n        \"\"\"Initialize cache with time-to-live.\n        \n        Args:\n            ttl_hours: How long to keep cached responses\n        \"\"\"\n        self.cache = {}  # In production: use Redis or file\n        self.ttl = timedelta(hours=ttl_hours)\n    \n    def _make_key(self, prompt, model):\n        \"\"\"Create cache key from prompt and model.\"\"\"\n        content = f\"{prompt}:{model}\"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def get(self, prompt, model):\n        \"\"\"Get cached response if it exists and is fresh.\"\"\"\n        key = self._make_key(prompt, model)\n        \n        if key in self.cache:\n            entry = self.cache[key]\n            age = datetime.now() - entry['timestamp']\n            \n            if age < self.ttl:\n                print(f\"âœ“ Cache hit! Saved API call\")\n                return entry['response']\n            else:\n                # Expired, remove it:\n                del self.cache[key]\n        \n        return None\n    \n    def set(self, prompt, model, response):\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(prompt, model)\n        self.cache[key] = {\n            'response': response,\n            'timestamp': datetime.now()\n        }\n        print(f\"Cached response (key: {key[:8]}...)\")\n    \n    def clear_expired(self):\n        \"\"\"Remove expired entries.\"\"\"\n        now = datetime.now()\n        expired = [\n            key for key, entry in self.cache.items()\n            if now - entry['timestamp'] >= self.ttl\n        ]\n        \n        for key in expired:\n            del self.cache[key]\n        \n        return len(expired)\n\n# Example usage:\ncache = AICache(ttl_hours=1)\n\n# First call - cache miss:\nresponse1 = cache.get(\"Explain Python\", \"claude-sonnet\")\nif response1 is None:\n    print(\"Cache miss, calling AI...\")\n    response1 = \"Python is a programming language...\"  # Simulated AI call\n    cache.set(\"Explain Python\", \"claude-sonnet\", response1)\n\n# Second call - cache hit:\nresponse2 = cache.get(\"Explain Python\", \"claude-sonnet\")\nprint(f\"Response: {response2[:30]}...\")",
      "explanation": "Caching saves money and speeds up your app! Same question = use cached answer instead of calling AI again."
    },

    {
      "type": "text",
      "text": "PATTERN 2: CONVERSATION HISTORY (STATEFUL AI)\n\nMaintain context across multiple exchanges like a real conversation:"
    },

    {
      "type": "code",
      "code": "# âœ… Conversation history pattern:\n\nclass AIConversation:\n    \"\"\"Manage a stateful conversation with AI.\"\"\"\n    \n    def __init__(self, system_prompt=\"\", max_history=10):\n        \"\"\"Initialize conversation.\n        \n        Args:\n            system_prompt: AI's role/behavior\n            max_history: Maximum messages to keep in history\n        \"\"\"\n        self.system_prompt = system_prompt\n        self.messages = []\n        self.max_history = max_history\n    \n    def add_user_message(self, content):\n        \"\"\"Add a user message.\"\"\"\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": content\n        })\n        self._trim_history()\n    \n    def add_assistant_message(self, content):\n        \"\"\"Add an AI response.\"\"\"\n        self.messages.append({\n            \"role\": \"assistant\",\n            \"content\": content\n        })\n        self._trim_history()\n    \n    def _trim_history(self):\n        \"\"\"Keep only recent messages to save tokens.\"\"\"\n        if len(self.messages) > self.max_history:\n            # Keep the most recent messages:\n            self.messages = self.messages[-self.max_history:]\n    \n    def get_messages_for_api(self):\n        \"\"\"Get messages formatted for API call.\"\"\"\n        # In real API call, include system prompt separately\n        return self.messages\n    \n    def get_summary(self):\n        \"\"\"Get conversation summary.\"\"\"\n        return {\n            \"total_messages\": len(self.messages),\n            \"user_messages\": sum(1 for m in self.messages if m['role'] == 'user'),\n            \"ai_messages\": sum(1 for m in self.messages if m['role'] == 'assistant')\n        }\n\n# Example usage:\nconvo = AIConversation(\n    system_prompt=\"You are a helpful Python tutor.\",\n    max_history=6\n)\n\n# Simulated conversation:\nconvo.add_user_message(\"What is a variable?\")\nconvo.add_assistant_message(\"A variable is a container for data...\")\n\nconvo.add_user_message(\"Can you show an example?\")\nconvo.add_assistant_message(\"Sure! x = 10 creates a variable...\")\n\nconvo.add_user_message(\"What about types?\")\n\nprint(\"Conversation history:\")\nfor msg in convo.get_messages_for_api():\n    print(f\"  {msg['role']}: {msg['content'][:40]}...\")\n\nprint(f\"\\nSummary: {convo.get_summary()}\")",
      "explanation": "Conversations need memory! Store message history so the AI remembers what was said. Trim old messages to save tokens."
    },

    {
      "type": "text",
      "text": "PATTERN 3: RETRY WITH EXPONENTIAL BACKOFF\n\nAPIs can fail. Retry intelligently with increasing wait times:"
    },

    {
      "type": "code",
      "code": "# âœ… Retry pattern with exponential backoff:\n\nimport time\nimport random\n\ndef retry_with_backoff(func, max_retries=3, base_delay=1):\n    \"\"\"Retry a function with exponential backoff.\n    \n    Args:\n        func: Function to retry\n        max_retries: Maximum number of retry attempts\n        base_delay: Initial delay in seconds\n    \n    Returns:\n        Function result or raises last exception\n    \"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        \n        except Exception as e:\n            if attempt == max_retries - 1:\n                # Last attempt failed, give up:\n                raise e\n            \n            # Calculate delay with exponential backoff:\n            delay = base_delay * (2 ** attempt)\n            # Add jitter (randomness) to avoid thundering herd:\n            jitter = random.uniform(0, delay * 0.1)\n            total_delay = delay + jitter\n            \n            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n            print(f\"Retrying in {total_delay:.2f}s...\")\n            \n            time.sleep(total_delay)\n    \n    return None\n\n# Example usage:\nattempt_count = 0\n\ndef flaky_api_call():\n    \"\"\"Simulates an unreliable API.\"\"\"\n    global attempt_count\n    attempt_count += 1\n    \n    # Fail first 2 attempts, succeed on 3rd:\n    if attempt_count < 3:\n        raise Exception(\"Rate limit exceeded\")\n    \n    return \"Success!\"\n\nprint(\"Testing retry pattern:\")\nresult = retry_with_backoff(flaky_api_call, max_retries=5)\nprint(f\"\\nFinal result: {result}\")",
      "explanation": "Exponential backoff: wait 1s, then 2s, then 4s, etc. Jitter adds randomness to avoid all retries hitting at once."
    },

    {
      "type": "text",
      "text": "PATTERN 4: BATCH PROCESSING\n\nProcess multiple items efficiently in batches:"
    },

    {
      "type": "code",
      "code": "# âœ… Batch processing pattern:\n\nfrom typing import List, Dict, Any\n\nclass AIBatchProcessor:\n    \"\"\"Process multiple items with AI efficiently.\"\"\"\n    \n    def __init__(self, batch_size=5, delay_between_batches=1.0):\n        \"\"\"Initialize batch processor.\n        \n        Args:\n            batch_size: Items per batch\n            delay_between_batches: Seconds to wait between batches\n        \"\"\"\n        self.batch_size = batch_size\n        self.delay = delay_between_batches\n    \n    def process_batch(self, items: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Process a single batch.\"\"\"\n        results = []\n        \n        for item in items:\n            # In real code: call AI API\n            # response = ai_client.generate(item)\n            \n            result = {\n                \"input\": item,\n                \"output\": f\"Processed: {item}\",\n                \"success\": True\n            }\n            results.append(result)\n        \n        return results\n    \n    def process_all(self, items: List[str]) -> List[Dict[str, Any]]:\n        \"\"\"Process all items in batches.\"\"\"\n        all_results = []\n        total_batches = (len(items) + self.batch_size - 1) // self.batch_size\n        \n        print(f\"Processing {len(items)} items in {total_batches} batches...\\n\")\n        \n        for i in range(0, len(items), self.batch_size):\n            batch_num = (i // self.batch_size) + 1\n            batch = items[i:i + self.batch_size]\n            \n            print(f\"Batch {batch_num}/{total_batches}: {len(batch)} items\")\n            \n            try:\n                results = self.process_batch(batch)\n                all_results.extend(results)\n                print(f\"  âœ“ Success: {len(results)} processed\")\n            \n            except Exception as e:\n                print(f\"  âœ— Error: {str(e)}\")\n                # Add failures to results:\n                for item in batch:\n                    all_results.append({\n                        \"input\": item,\n                        \"output\": None,\n                        \"success\": False,\n                        \"error\": str(e)\n                    })\n            \n            # Wait between batches (rate limiting):\n            if batch_num < total_batches:\n                time.sleep(self.delay)\n        \n        return all_results\n    \n    def get_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Get processing summary.\"\"\"\n        successful = sum(1 for r in results if r['success'])\n        failed = len(results) - successful\n        \n        return {\n            \"total\": len(results),\n            \"successful\": successful,\n            \"failed\": failed,\n            \"success_rate\": (successful / len(results) * 100) if results else 0\n        }\n\n# Example usage:\nprocessor = AIBatchProcessor(batch_size=3, delay_between_batches=0.5)\n\nitems = [\n    \"Explain variables\",\n    \"Explain functions\",\n    \"Explain loops\",\n    \"Explain classes\",\n    \"Explain modules\",\n    \"Explain decorators\",\n    \"Explain generators\"\n]\n\nresults = processor.process_all(items)\nsummary = processor.get_summary(results)\n\nprint(f\"\\nSummary: {summary}\")",
      "explanation": "Batch processing: divide items into chunks, process each chunk, wait between batches. Efficient for large workloads!"
    },

    {
      "type": "tip",
      "text": "Pro tip: Combine patterns! Use caching + batch processing, or retry + conversation history. These patterns are building blocks."
    },

    {
      "type": "text",
      "text": "PATTERN 5: STREAMING RESPONSES (REAL-TIME OUTPUT)\n\nFor long responses, show text as it's generated instead of waiting:"
    },

    {
      "type": "code",
      "code": "# ðŸ“‹ Streaming pattern (for real AI APIs):\n\nimport anthropic\nimport sys\n\ndef stream_ai_response(prompt, api_key):\n    \"\"\"Stream AI response in real-time.\n    \n    This shows output as it's generated instead of waiting\n    for the complete response.\n    \"\"\"\n    client = anthropic.Anthropic(api_key=api_key)\n    \n    # Use stream=True:\n    with client.messages.stream(\n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    ) as stream:\n        # Print each chunk as it arrives:\n        for text in stream.text_stream:\n            print(text, end=\"\", flush=True)\n    \n    print()  # New line at end\n\n# In the browser, we simulate streaming:\ndef simulate_streaming(text, chunk_size=10):\n    \"\"\"Simulate streaming by printing in chunks.\"\"\"\n    for i in range(0, len(text), chunk_size):\n        chunk = text[i:i + chunk_size]\n        print(chunk, end=\"\", flush=True)\n        time.sleep(0.05)  # Simulate network delay\n    print()\n\nlong_text = \"Python is a high-level programming language that emphasizes code readability and simplicity. It's widely used in web development, data science, and automation.\"\n\nprint(\"Simulated streaming:\")\nsimulate_streaming(long_text, chunk_size=5)",
      "explanation": "Streaming gives instant feedback! Instead of waiting 10 seconds for full response, users see text appearing immediately."
    },

    {
      "type": "text",
      "text": "PATTERN 6: FALLBACK STRATEGY (MULTIPLE AI PROVIDERS)\n\nIf one AI fails or is expensive, fall back to alternatives:"
    },

    {
      "type": "code",
      "code": "# âœ… Fallback pattern:\n\nclass AIFallbackStrategy:\n    \"\"\"Try multiple AI providers in order.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize with provider priority list.\"\"\"\n        self.providers = [\n            {\"name\": \"claude\", \"cost\": \"medium\", \"available\": True},\n            {\"name\": \"gpt-4\", \"cost\": \"high\", \"available\": True},\n            {\"name\": \"gpt-3.5\", \"cost\": \"low\", \"available\": True}\n        ]\n    \n    def call_provider(self, provider_name, prompt):\n        \"\"\"Call a specific AI provider.\"\"\"\n        # In real code: actual API calls\n        # For now, simulate some failures:\n        \n        if provider_name == \"claude\":\n            # Simulate rate limit:\n            raise Exception(\"Rate limit exceeded\")\n        elif provider_name == \"gpt-4\":\n            # Works!\n            return f\"Response from {provider_name}: {prompt[:20]}...\"\n        else:\n            return f\"Response from {provider_name}: {prompt[:20]}...\"\n    \n    def generate(self, prompt, max_cost=\"high\"):\n        \"\"\"Generate response with fallback.\n        \n        Args:\n            prompt: The prompt to send\n            max_cost: Maximum cost tier to use\n        \n        Returns:\n            AI response or error\n        \"\"\"\n        cost_tiers = {\"low\": 1, \"medium\": 2, \"high\": 3}\n        max_cost_level = cost_tiers[max_cost]\n        \n        # Filter providers by cost:\n        eligible = [\n            p for p in self.providers\n            if cost_tiers[p['cost']] <= max_cost_level and p['available']\n        ]\n        \n        # Try each provider:\n        errors = []\n        for provider in eligible:\n            try:\n                print(f\"Trying {provider['name']}...\")\n                response = self.call_provider(provider['name'], prompt)\n                print(f\"âœ“ Success with {provider['name']}\")\n                return response\n            \n            except Exception as e:\n                error_msg = f\"{provider['name']}: {str(e)}\"\n                errors.append(error_msg)\n                print(f\"âœ— {error_msg}\")\n                continue\n        \n        # All providers failed:\n        return f\"All providers failed:\\n\" + \"\\n\".join(errors)\n\n# Example usage:\nfallback = AIFallbackStrategy()\n\nresult = fallback.generate(\"Explain Python variables\", max_cost=\"high\")\nprint(f\"\\nFinal result: {result}\")",
      "explanation": "Don't rely on a single AI! If Claude is rate-limited, try GPT. If GPT-4 is too expensive, try GPT-3.5. Resilient systems have fallbacks."
    },

    {
      "type": "text",
      "text": "PATTERN 7: PROMPT CHAINING (MULTI-STEP AI WORKFLOWS)\n\nBreak complex tasks into steps, where each AI call builds on the previous:"
    },

    {
      "type": "code",
      "code": "# âœ… Prompt chaining pattern:\n\nclass AIWorkflow:\n    \"\"\"Chain multiple AI calls for complex tasks.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize workflow.\"\"\"\n        self.steps_completed = []\n    \n    def execute_step(self, step_name, prompt):\n        \"\"\"Execute a single workflow step.\"\"\"\n        print(f\"\\nStep: {step_name}\")\n        print(f\"Prompt: {prompt[:50]}...\")\n        \n        # In real code: call AI\n        response = f\"[AI response for {step_name}]\"\n        \n        self.steps_completed.append({\n            \"step\": step_name,\n            \"prompt\": prompt,\n            \"response\": response\n        })\n        \n        return response\n    \n    def analyze_code_workflow(self, code):\n        \"\"\"Multi-step code analysis workflow.\"\"\"\n        # Step 1: Understand what the code does:\n        step1 = self.execute_step(\n            \"Understand\",\n            f\"Explain what this code does in one sentence:\\n{code}\"\n        )\n        \n        # Step 2: Find issues (uses result from step 1):\n        step2 = self.execute_step(\n            \"Find Issues\",\n            f\"Given that this code {step1}, list any bugs or issues:\\n{code}\"\n        )\n        \n        # Step 3: Suggest improvements (uses results from steps 1 & 2):\n        step3 = self.execute_step(\n            \"Improve\",\n            f\"This code {step1}. Issues found: {step2}.\\n\\nProvide improved version:\\n{code}\"\n        )\n        \n        # Step 4: Explain changes:\n        step4 = self.execute_step(\n            \"Explain Changes\",\n            f\"Explain the improvements made from the original to the new version.\"\n        )\n        \n        return {\n            \"understanding\": step1,\n            \"issues\": step2,\n            \"improved_code\": step3,\n            \"explanation\": step4\n        }\n    \n    def get_workflow_summary(self):\n        \"\"\"Get workflow execution summary.\"\"\"\n        return {\n            \"total_steps\": len(self.steps_completed),\n            \"steps\": [s['step'] for s in self.steps_completed]\n        }\n\n# Example usage:\nworkflow = AIWorkflow()\n\ncode_sample = \"def calc(x,y): return x+y\"\nresult = workflow.analyze_code_workflow(code_sample)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"WORKFLOW COMPLETE\")\nprint(\"=\"*50)\nprint(f\"Summary: {workflow.get_workflow_summary()}\")",
      "explanation": "Chaining: each AI call uses results from previous calls. Like a pipeline: step1 â†’ step2 â†’ step3. Great for complex tasks!"
    },

    {
      "type": "exercise",
      "prompt": "Create a SmartCache class that combines caching with expiration and usage statistics. It should: 1) Cache AI responses with timestamps, 2) Track cache hits/misses, 3) Automatically clean expired entries, 4) Provide statistics (hit rate, total savings).",
      "starter": "from datetime import datetime, timedelta\nimport hashlib\n\nclass SmartCache:\n    \"\"\"Advanced caching with statistics.\"\"\"\n    \n    def __init__(self, ttl_minutes=60):\n        \"\"\"Initialize cache.\n        \n        Args:\n            ttl_minutes: Time-to-live in minutes\n        \"\"\"\n        # Your code here:\n        # - self.cache = {}\n        # - self.ttl\n        # - self.hits = 0\n        # - self.misses = 0\n        pass\n    \n    def _make_key(self, prompt):\n        \"\"\"Create cache key from prompt.\"\"\"\n        pass\n    \n    def get(self, prompt):\n        \"\"\"Get cached response (return None if miss/expired).\"\"\"\n        pass\n    \n    def set(self, prompt, response):\n        \"\"\"Cache a response.\"\"\"\n        pass\n    \n    def get_stats(self):\n        \"\"\"Return statistics dictionary.\"\"\"\n        pass\n\n# Test it:\ncache = SmartCache(ttl_minutes=1)\n\n# Test cache miss:\nresult1 = cache.get(\"What is Python?\")\nprint(f\"First call: {result1}\")  # None\n\n# Cache it:\ncache.set(\"What is Python?\", \"Python is a programming language\")\n\n# Test cache hit:\nresult2 = cache.get(\"What is Python?\")\nprint(f\"Second call: {result2}\")  # Should return cached value\n\n# Get statistics:\nstats = cache.get_stats()\nprint(f\"Stats: {stats}\")",
      "solution": "from datetime import datetime, timedelta\nimport hashlib\n\nclass SmartCache:\n    \"\"\"Advanced caching with statistics.\"\"\"\n    \n    def __init__(self, ttl_minutes=60):\n        \"\"\"Initialize cache.\n        \n        Args:\n            ttl_minutes: Time-to-live in minutes\n        \"\"\"\n        self.cache = {}  # key -> {response, timestamp}\n        self.ttl = timedelta(minutes=ttl_minutes)\n        self.hits = 0\n        self.misses = 0\n        self.api_calls_saved = 0\n    \n    def _make_key(self, prompt):\n        \"\"\"Create cache key from prompt.\"\"\"\n        return hashlib.md5(prompt.encode()).hexdigest()\n    \n    def _is_expired(self, timestamp):\n        \"\"\"Check if timestamp is expired.\"\"\"\n        age = datetime.now() - timestamp\n        return age >= self.ttl\n    \n    def get(self, prompt):\n        \"\"\"Get cached response (return None if miss/expired).\"\"\"\n        key = self._make_key(prompt)\n        \n        if key in self.cache:\n            entry = self.cache[key]\n            \n            if self._is_expired(entry['timestamp']):\n                # Expired - remove and count as miss:\n                del self.cache[key]\n                self.misses += 1\n                return None\n            else:\n                # Valid cache hit:\n                self.hits += 1\n                self.api_calls_saved += 1\n                return entry['response']\n        else:\n            # Cache miss:\n            self.misses += 1\n            return None\n    \n    def set(self, prompt, response):\n        \"\"\"Cache a response.\"\"\"\n        key = self._make_key(prompt)\n        self.cache[key] = {\n            'response': response,\n            'timestamp': datetime.now(),\n            'prompt': prompt[:50] + '...' if len(prompt) > 50 else prompt\n        }\n    \n    def clean_expired(self):\n        \"\"\"Remove expired entries.\"\"\"\n        now = datetime.now()\n        expired_keys = [\n            key for key, entry in self.cache.items()\n            if self._is_expired(entry['timestamp'])\n        ]\n        \n        for key in expired_keys:\n            del self.cache[key]\n        \n        return len(expired_keys)\n    \n    def get_stats(self):\n        \"\"\"Return statistics dictionary.\"\"\"\n        total_requests = self.hits + self.misses\n        hit_rate = (self.hits / total_requests * 100) if total_requests > 0 else 0\n        \n        # Estimate cost savings (assume $0.01 per API call):\n        cost_per_call = 0.01\n        estimated_savings = self.api_calls_saved * cost_per_call\n        \n        return {\n            'total_requests': total_requests,\n            'cache_hits': self.hits,\n            'cache_misses': self.misses,\n            'hit_rate_percent': round(hit_rate, 2),\n            'cached_entries': len(self.cache),\n            'api_calls_saved': self.api_calls_saved,\n            'estimated_savings_usd': round(estimated_savings, 2)\n        }\n    \n    def clear(self):\n        \"\"\"Clear all cache.\"\"\"\n        self.cache.clear()\n\n# Test it:\nprint(\"Testing SmartCache\")\nprint(\"=\"*50)\n\ncache = SmartCache(ttl_minutes=60)\n\n# Test 1: Cache miss\nprint(\"\\nTest 1: Cache miss\")\nresult1 = cache.get(\"What is Python?\")\nprint(f\"Result: {result1}\")\nprint(f\"Stats: {cache.get_stats()}\")\n\n# Test 2: Cache the response\nprint(\"\\nTest 2: Caching response\")\ncache.set(\"What is Python?\", \"Python is a programming language\")\n\n# Test 3: Cache hit\nprint(\"\\nTest 3: Cache hit\")\nresult2 = cache.get(\"What is Python?\")\nprint(f\"Result: {result2}\")\n\n# Test 4: Another cache hit\nresult3 = cache.get(\"What is Python?\")\nprint(f\"Result: {result3}\")\n\n# Test 5: Different prompt (cache miss)\nprint(\"\\nTest 5: Different prompt\")\nresult4 = cache.get(\"What is JavaScript?\")\nprint(f\"Result: {result4}\")\n\n# Final statistics:\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL STATISTICS\")\nprint(\"=\"*50)\nstats = cache.get_stats()\nfor key, value in stats.items():\n    print(f\"{key}: {value}\")",
      "hint": "Track hits/misses as instance variables. In get(): if key exists and not expired, increment hits and return cached value; otherwise increment misses. In get_stats(): calculate hit_rate = hits / (hits + misses) * 100."
    }
  ],

  "nextLesson": "09-ai-code-reviewer",
  "prevLesson": "07-environment-best-practices"
}
